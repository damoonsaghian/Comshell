the problem of shared mutable data is so pervasive in the computing world;
we can also see it in file synchronization tools like Git:
, there can be conflicts when pushing or pulling, which needs to be resolved manually;
, the history just grows indefinitely cause change in the history of the repository can be catastrophic;

solution:
only the owner can mutate the repository;
each file or directory can have an owner;
owner can mutably borrow its content for a defined duration, or move ownership;
others have to send messages:
, in the form of to_dos defined by the owner;
, corrections

immutable repositories can apply corrections and to_dos, and test them,
  but they can't push it to remote, only the owner can;
automatic sync at the immutable ends;
sync before sending to_dos or corrections;

a WebDAVS or FTPS server for remote storage;
why not SFTP? because SSH fingerprint verification is complicated; we have 2 chocies:
, DNSSEC: in addition to being complex and not widely deployed,
  the identity verification is done at the recursive resolvers, not a the person's computer;
  so the resolver must be trusted;
  "https://www.redpill-linpro.com/techblog/2019/05/06/sshfp-and-dnssec.html"
  "https://www.cloudflare.com/dns/dnssec/how-dnssec-works/"
, using a separate HTTPS URL, which can add a little complexity;
why not cloud storage services?
  cause they have no proper way to present a URL of the repository to the public;

write a file synchronization in Rust or NodeJS;
files are scanned and their paths, hashes and modification times are stored in an index file;
this index file is compared with the one in the destination;
a list of the files which must be removed is written in the destination
  (if there is an old one, it will be appended to it);
changed files and new files are written to the root directory of the destination,
  using their hash as the file name;
the index file is written to the destination;
the files which must be removed will be deleted;
to download from the destination, first

chunking files is useless; mutable files must be kept small;
databases are bad for syncing; chunking does not work for them;
we can implement an algorithm for each type of database;
but diffing is bad idea, because it doubles the needed storage;

alternatives and there problems:
Git doubles the storage needed:
, working directoy;
, local repository (or the cache directory if you use Git LFS);
Rsync, Rdiff-backup, Duplicity: they don't handle renames, and they only support SSH;
Unison: it handles renames but instead of moving it copies and deletes in remote, and only supports SSH;
Bup, Borg, Snebu: they handle renames, but only supports SSH;
Rclone: it handles renames only when using SFTP or cloud storage; in addition:
, inconsistent repository is possible, unless a lock mechanism is implemented;
, download/upload resume not supported (since it does not split files to chunks, it can be problematic);
Restic:
, weired UI (eg one has to type repository every time);
, can't disable encryption;
, locks repository when mutating;
, prune is very slow and it locks repository;
, high CPU and memory usage;
Kopia: weird UI (what does "connecting to a repo" means)
knoxite: weired UI (what's the point of volumes)
Duplicacy: really good; consistent, lockless and simple repository;
  i think the only problem is it's license;

files in ".cache" directory will no be synced;
so build directories must be put inside ".cache"; eg for Rust:
; nano ~/.cargo/config
  [build]
  target-dir = ".cache/cargo"
